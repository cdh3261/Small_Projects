# 인공지능 기초 이론(4)

### Deep Natural Language Processing 

Word Embedding 단어 하나를 가지고 벡터로 표현하는것.



단어를 벡터로 나타내고 비슷한 단어는 비슷한 위치에 있다.
ex) cat -> cats 와 dog -> dogs는 같은 방향을 가진다.

단어에서 문장, 문단 이렇게 늘어난다.

뉴럴로 단어를 학습한다! 방향성으로=벡터 !

같은 문장에서 나오는 단어는 비슷할 수 있다. ( _____ is cute. ___에 들어갈 단어는 비슷할 것이다.)



#### CBOW 

문맥에 있는 단어가 주어지고 중간에 있는 단어가 무엇인가 유추



#### Skip_Gram

CBOW와 다르게 중간 단어가 주어지고 양 옆의 단어가 무엇인가 유추



#### word analogy task

위치하고 있는 단어에서 가장 가까운 단어를 얼마나 잘 찾아내는지



#### word vector

단어 학습을 많이 하지 않으면 좋지 않을 수도 있다. =>단어뿐 아니라 다른 유닛을 기준으로 하면된다. 

=> Subword Information Skip-Gram => where 을 wh, ere, re... 이렇게 나누어 학습시킨다.



#### 엘모와 버트

최근에는 엘모와 버트라는 방법을 사용한다. word에는 문장이 없었다. 엘모와 버트는 context를 기반으로 한 방향성이 한쪽인 방법이다.